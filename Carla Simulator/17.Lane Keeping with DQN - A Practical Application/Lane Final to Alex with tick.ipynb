{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e9e7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mostafa\\AppData\\Local\\Temp\\ipykernel_17200\\1663751618.py:79: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                    | 0/150 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mostafa\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|#3                                                                   | 3/150 [1:34:47<77:24:24, 1895.68s/episodes]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "from threading import Thread\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image,ImageFilter\n",
    "\n",
    "from test_model import testModel\n",
    "\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('E:\\\\CARLA_0.9.14\\\\WindowsNoEditor\\\\PythonAPI\\\\carla\\\\dist\\\\carla-0.9.14-py3.7-win-amd64.egg')[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla\n",
    "\n",
    "\n",
    "\n",
    "FPS=10\n",
    "SHOW_PREVIEW = False\n",
    "IM_WIDTH = 1024\n",
    "IM_HEIGHT = 720\n",
    "training_width = 256\n",
    "training_height = 128\n",
    "SECONDS_PER_EPISODE = 10\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 16\n",
    "MINIBATCH_SIZE = 16\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "MODEL_NAME = \"Xception\"\n",
    "\n",
    "MEMORY_FRACTION = 0.4\n",
    "MIN_REWARD = -100\n",
    "\n",
    "EPISODES = 150\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "epsilon = 0.001\n",
    "EPSILON_DECAY = 0.9975 ## 0.9975 99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "\n",
    "state_size = (128,256,3)\n",
    "\n",
    "\n",
    "action_size =3\n",
    "actions_steer=[0.15,-0.15,0]\n",
    "actions_speed=[0.7]\n",
    "\n",
    "\n",
    "SHOW = False\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "        self.step+=1\n",
    "\n",
    "\n",
    "class CarEnv:\n",
    "    SHOW_CAM = SHOW_PREVIEW\n",
    "    STEER_AMT = 1.0\n",
    "    im_width = IM_WIDTH\n",
    "    im_height = IM_HEIGHT\n",
    "    front_camera = None\n",
    "    action = None\n",
    "    \n",
    "    fixed_delta_seconds = 0.1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = carla.Client(\"localhost\", 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.get_world()\n",
    "\n",
    "        traffic_manager = self.client.get_trafficmanager(8000)\n",
    "        traffic_manager.set_synchronous_mode(True)\n",
    "\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = self.fixed_delta_seconds\n",
    "        settings.no_rendering_mode = True\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.model_3 = self.blueprint_library.filter(\"vehicle.carlamotors.firetruck\")[0]\n",
    "\n",
    "    def reset(self):\n",
    "        self.list_lane = []\n",
    "        self.actor_list = []\n",
    "        self.collision_event=0\n",
    "\n",
    "        self.transform = self.world.get_map().get_spawn_points()[random.randint(0,79)]\n",
    "        self.vehicle = self.world.spawn_actor(self.model_3, self.transform)\n",
    "        self.actor_list.append(self.vehicle)\n",
    "\n",
    "        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        self.rgb_cam.set_attribute(\"image_size_x\", f\"{self.im_width}\")\n",
    "        self.rgb_cam.set_attribute(\"image_size_y\", f\"{self.im_height}\")\n",
    "        self.rgb_cam.set_attribute(\"fov\", f\"90\")\n",
    "\n",
    "        transform = carla.Transform(carla.Location(x=4,z=1.6))\n",
    "\n",
    "        self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.sensor)\n",
    "        self.sensor.listen(lambda data: self.process_img(data))\n",
    "\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        #time.sleep(4)\n",
    "\n",
    "        #lanesensor = self.blueprint_library.find(\"sensor.other.lane_invasion\")\n",
    "        #self.Lane_sensor = self.world.spawn_actor(lanesensor, transform, attach_to=self.vehicle)\n",
    "        #self.actor_list.append(self.Lane_sensor)\n",
    "        #self.Lane_sensor.listen(lambda event: self.lane_data(event.crossed_lane_markings))\n",
    "        \n",
    "        colsensor = self.blueprint_library.find(\"sensor.other.collision\")\n",
    "        self.collision = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)\n",
    "        self.actor_list.append(self.collision)\n",
    "        self.collision.listen(lambda event: self.collisionFunc(event))\n",
    "\n",
    "        while self.front_camera is None:\n",
    "            self.world.tick()\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "       \n",
    "\n",
    "        #self.episode_start = time.time()\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0))\n",
    "        #self.world.tick()\n",
    "        #time.sleep(self.fixed_delta_seconds)\n",
    "    \n",
    "        return self.front_camera\n",
    "\n",
    "    def lane_data(self,lanes):\n",
    "        for marking in lanes:\n",
    "            self.list_lane.append(marking.type)\n",
    "\n",
    "    def process_img(self, image):\n",
    "        i = np.array(image.raw_data)\n",
    "        #print(i.shape)\n",
    "        i2 = i.reshape((self.im_height, self.im_width, 4))\n",
    "        i3 = i2[:, :, :3]\n",
    "        if self.SHOW_CAM:\n",
    "            cv2.imshow(\"\", i3)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        #img=Image.fromarray(i3)\n",
    "        #res=img.filter(ImageFilter.CONTOUR())\n",
    "\n",
    "        self.front_camera = np.array(i3)\n",
    "        \n",
    "       \n",
    "    \n",
    "    def collisionFunc(self,e):\n",
    "        self.collision_event=1\n",
    "\n",
    "    \n",
    "    def step(self,action_steer,action_speed):\n",
    "        done=False\n",
    "        reward=0\n",
    "        \n",
    "        #self.world.tick()\n",
    "        #time.sleep(self.fixed_delta_seconds)\n",
    "        #vehicle_location = self.vehicle.get_location()\n",
    "        \n",
    "    \n",
    "\n",
    "        # if action_speed==0:\n",
    "        #     reward=20\n",
    "        # else:\n",
    "        #     reward=-20\n",
    "        \n",
    "        \n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=actions_speed[action_speed],brake=0,steer=actions_steer[action_steer],reverse=0))\n",
    "#         action = round(self.vehicle.get_control().steer,1)\n",
    "#         action = actions_steer.index(action)\n",
    "#         s = time.time()\n",
    "#         while time.time() - s < 0.1:\n",
    "        self.world.tick()\n",
    "        #time.sleep(self.fixed_delta_seconds)\n",
    "        #print(\"input: \",actions_steer[action_steer])\n",
    "        \n",
    "#         print(\"out: \",action)\n",
    "#         min_d = 2\n",
    "#         index_action = None\n",
    "#         cnt=0\n",
    "#         for ac in actions_steer:\n",
    "#             if abs(action - ac) < min_d:\n",
    "#                 min_d = action - ac\n",
    "#                 index_action = cnt\n",
    "#             cnt+=1\n",
    "#         action = index_action\n",
    "        #print(\"out: \",action)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.world.tick()\n",
    "#         time.sleep(self.fixed_delta_seconds)\n",
    "        \n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        waypoint_road = self.world.get_map().get_waypoint(vehicle_location, project_to_road=True, \n",
    "                    lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        x_wp = waypoint_road.transform.location.x\n",
    "        y_wp = waypoint_road.transform.location.y\n",
    "\n",
    "        x_vh = vehicle_location.x\n",
    "        y_vh = vehicle_location.y\n",
    "\n",
    "        wp_array = np.array([x_wp, y_wp])\n",
    "        vh_array = np.array([x_vh, y_vh])\n",
    "\n",
    "        dist = np.linalg.norm(wp_array - vh_array)\n",
    "        #print(\"distance= \",dist)\n",
    "\n",
    "        waypoint_vehicle_yaw=self.vehicle.get_transform().rotation.yaw\n",
    "        waypoint_vehicle_yaw_rad = math.radians(waypoint_vehicle_yaw)\n",
    "        waypoint_vehicle_yaw_rad = waypoint_vehicle_yaw_rad % (2 * math.pi)\n",
    "\n",
    "        waypoint_road_yaw = waypoint_road.transform.rotation.yaw\n",
    "        waypoint_road_yaw_rad = math.radians(waypoint_road_yaw)\n",
    "        waypoint_road_yaw_rad = waypoint_road_yaw_rad % (2 * math.pi)\n",
    "\n",
    "        diff_yaw_rad = waypoint_vehicle_yaw_rad - waypoint_road_yaw_rad\n",
    "        #diff_yaw_deg = diff_yaw_rad*180/math.pi\n",
    "        #print(\"yaw= \",diff_yaw_deg)\n",
    "\n",
    "        cos_yaw_diff=np.cos(diff_yaw_rad)\n",
    "        reward = (1 * cos_yaw_diff) - (1 * dist) - (5*int(self.collision_event))\n",
    "        \n",
    "        #print(reward)\n",
    "        \n",
    "        if self.collision_event:\n",
    "            done=True\n",
    "            self.collision_event=0\n",
    "        #print(\"reward= \",reward)\n",
    "        \n",
    "        #time.sleep(1.5)\n",
    "\n",
    "        #print(self.list_lane)\n",
    "\n",
    "#         for marking in self.list_lane:\n",
    "#             if marking==carla.libcarla.LaneMarkingType.Broken or marking==carla.libcarla.LaneMarkingType.BrokenBroken :\n",
    "#                 self.done=True\n",
    "#                 done=True\n",
    "#                 break\n",
    "#                 reward=-10\n",
    "\n",
    "#             elif marking==carla.libcarla.LaneMarkingType.Solid or marking==carla.libcarla.LaneMarkingType.Curb or marking==carla.libcarla.LaneMarkingType.Grass \\\n",
    "#                 or marking==carla.libcarla.LaneMarkingType.SolidSolid or marking==carla.libcarla.LaneMarkingType.SolidBroken or marking==carla.libcarla.LaneMarkingType.BrokenSolid:\n",
    "#                 self.done=True\n",
    "#                 done=True\n",
    "#                 break\n",
    "\n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.Broken)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.Broken)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.BrokenBroken)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.BrokenBroken)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.Solid)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.Solid)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.Curb)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.Curb)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.Grass)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.Grass)\n",
    "\n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.SolidSolid)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.SolidSolid)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.SolidBroken)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.SolidBroken)\n",
    "        \n",
    "#         for _ in range(self.list_lane.count(carla.libcarla.LaneMarkingType.BrokenSolid)):\n",
    "#             self.list_lane.remove(carla.libcarla.LaneMarkingType.BrokenSolid)\n",
    "        \n",
    "#        self.list_lane=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        return self.front_camera, reward, done\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\n",
    "        self.target_update_counter = 0\n",
    "        self.graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "        \n",
    "        \n",
    "        #self.reget()\n",
    "\n",
    "    def create_model(self):\n",
    "        base_model = Xception(weights=None, include_top=False, input_shape=(training_height, training_width,3))\n",
    "\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "        predictions = Dense(self.action_size, activation=\"linear\")(x)\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            #print(\"Return\")\n",
    "            return\n",
    "        #print(1)\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        with self.graph.as_default():\n",
    "            current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "        with self.graph.as_default():\n",
    "            future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        log_this_step = False\n",
    "        if self.tensorboard.step > self.last_logged_episode:\n",
    "            log_this_step = True\n",
    "            self.last_log_episode = self.tensorboard.step\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.model.fit(np.array(X)/255, np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=1, shuffle=False, callbacks=[self.tensorboard] if log_this_step else None)\n",
    "            #print(\"fitting\")\n",
    "\n",
    "        if log_this_step:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        norm_state=np.array(state.reshape(-1,*state.shape))/255\n",
    "        return self.model.predict(norm_state)[0]\n",
    "\n",
    "    def train_in_loop(self):\n",
    "        X = np.random.uniform(size=(1, training_height, training_width, 3)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, self.action_size)).astype(np.float32)\n",
    "        with self.graph.as_default():\n",
    "            self.model.fit(X,y, verbose=1, batch_size=1)\n",
    "\n",
    "        self.training_initialized = True\n",
    "\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.01)\n",
    "    def reget(self):\n",
    "        self.model.load_weights(r\"E:\\study\\graduation-proj\\Carla-Imitation\\ImitationFinal\\PretrainStep #Done\\PretrainedModelMaskServer10Epochs.ckpt\")\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    FPS = 10\n",
    "    # For stats\n",
    "    ep_rewards = [-1]\n",
    "\n",
    "    # For more repetitive results\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "\n",
    "    # Memory fraction, used mostly when trai8ning multiple agents\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "    #backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "    # Create models folder\n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # Create agent and environment\n",
    "    agent = DQNAgent(state_size,action_size)\n",
    "    env = CarEnv()\n",
    "    \n",
    "    if SHOW:\n",
    "        if not os.path.exists(\"Carla-Data-Test\"):\n",
    "            os.mkdir(\"Carla-Data-Test\")\n",
    "\n",
    "\n",
    "    # Start training thread and wait for training to be initialized\n",
    "    trainer_thread = Thread(target=agent.train_in_loop, daemon=True)\n",
    "    trainer_thread.start()\n",
    "    while not agent.training_initialized:\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Initialize predictions - forst prediction takes longer as of initialization that has to be done\n",
    "    # It's better to do a first prediction then before we start iterating over episode steps\n",
    "    #agent.get_qs(np.ones((env.im_height, env.im_width, 3)))\n",
    "    \n",
    "    num=1\n",
    "    # Iterate over episodes\n",
    "    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "        #try:\n",
    "            finish = False\n",
    "            env.list_lane = []\n",
    "\n",
    "            # Update tensorboard step every episode\n",
    "            agent.tensorboard.step = episode\n",
    "\n",
    "            # Restarting episode - reset episode reward and step number\n",
    "            episode_reward = 0\n",
    "            step = 1\n",
    "            \n",
    "            \n",
    "            env.front_camera=None\n",
    "        \n",
    "            # Reset environment and get initial state\n",
    "            current_state = env.reset()\n",
    "            current_state=testModel(current_state)\n",
    "            current_state = np.array(Image.fromarray(current_state).resize((training_width,training_height)))\n",
    "            \n",
    "            #cv2.imwrite(f'D:\\\\jupyter\\\\lanenet-lane-detection-pytorch-main\\\\mostafa\\\\1.png', current_state)\n",
    "\n",
    "            #current_state = np.expand_dims(current_state.reshape(256, 512, 1), axis=0)\n",
    "\n",
    "            # Reset flag and start iterating until episode ends\n",
    "            done = False\n",
    "            #episode_start = time.time()\n",
    "            \n",
    "\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "                \n",
    "                num+=1\n",
    "                if SHOW:\n",
    "                    str_num=str(num)\n",
    "                    while len(str_num)<5:\n",
    "                        str_num=\"0\"+str_num\n",
    "\n",
    "                    os.mkdir(\"Carla-Data-Test\\\\\"+str_num)\n",
    "                    cv2.imwrite(\"Carla-Data-Test\\\\\"+str_num+\"\\\\current.png\", current_state)\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                rand = np.random.random()\n",
    "                flag=0\n",
    "                if rand > epsilon:\n",
    "                    # Get action from Q table\n",
    "                    action = np.argmax(agent.get_qs(current_state))\n",
    "                    flag=1\n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, action_size)\n",
    "                    \n",
    "                    # This takes no time, so we add a delay matching 60 FPS (prediction above takes longer)\n",
    "                    \n",
    "\n",
    "                #vehicle_location = env.vehicle.get_location()\n",
    "                \n",
    "\n",
    "                new_state, reward, done = env.step(action,0) #steer,speed\n",
    "#                 cv2.imshow(\"current\",current_state)\n",
    "#                 cv2.waitKey(0)\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 print(actions_steer[action%7])\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                new_state = testModel(new_state)\n",
    "                new_state = np.array(Image.fromarray(new_state).resize((training_width,training_height)))\n",
    "                \n",
    "#                 cv2.imshow(\"next\",new_state)\n",
    "#                 cv2.waitKey(0)\n",
    "#                 cv2.destroyAllWindows()\n",
    "                if SHOW:\n",
    "                    cv2.imwrite(\"Carla-Data-Test\\\\\"+str_num+\"\\\\next.png\", new_state)\n",
    "                    with open(\"Carla-Data-Test\\\\\"+str_num+\"\\\\info.txt\",'w') as f1:\n",
    "                        f1.write(str(reward)+'\\n')\n",
    "                        f1.write(str(done)+'\\n')\n",
    "                        f1.write(str(actions_steer[action])+'\\n') #steer\n",
    "                        f1.write(str(actions_speed[0])+'\\n') #speed\n",
    "                        if flag==0:\n",
    "                            f1.write(\"random\")\n",
    "                        else:\n",
    "                            f1.write(\"exact\")\n",
    "                \n",
    "                #cv2.imwrite(f'D:\\\\jupyter\\\\lanenet-lane-detection-pytorch-main\\\\mostafa\\\\{num}.png', new_state)\n",
    "\n",
    "                #new_state = np.expand_dims(new_state.reshape(256, 512, 1), axis=0)\n",
    "\n",
    "\n",
    "                # Transform new continous state to new discrete state and count reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Every step we update replay memory\n",
    "                agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "\n",
    "                current_state = new_state\n",
    "                step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            # End of episode - destroy agents\n",
    "            for actor in env.actor_list:\n",
    "                actor.destroy()\n",
    "\n",
    "            # Append episode reward to a list and log stats (every given number of episodes)\n",
    "            ep_rewards.append(episode_reward)\n",
    "            if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "                average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "                agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "                # Save model, but only when min reward is greater or equal a set value\n",
    "#                 if min_reward >= MIN_REWARD:\n",
    "#                     agent.model.save_weights(f\"models\\\\{num}.h5\")\n",
    "#                     pass\n",
    "\n",
    "            # Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "                print(epsilon)\n",
    "            \n",
    "    # Set termination flag for training thread and wait for it to finish\n",
    "    agent.terminate = True\n",
    "    trainer_thread.join()\n",
    "    agent.model.save_weights(\"final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943997dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "a=deque(maxlen=2)\n",
    "a.append(1)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bd0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "# Open a file and use dump() \n",
    "with open('ReplayMemoryData.pkl', 'wb') as file: \n",
    "    pickle.dump(agent.replay_memory, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170813b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
